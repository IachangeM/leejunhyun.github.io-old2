---
layout: post
title: '(작성중)[Paper] Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation'
categories:
  - Deep Learning
  - BioMedical
tags:
  - deep learning
  - geometric deep learning
  - reinforcement learning
  - generative adversarial network
  - drug discovery
  - paper review
excerpt_separator: <!--more-->
---

RL(policy gradient) + GCN + GAN으로 drug discovery를 하는 논문입니다.

[Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation](https://arxiv.org/pdf/1806.02473.pdf),<br>
Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec, NIPS 2018
<!--more-->

* Abstract
* 1 Introduction
    * Present Work
    * Graph representation
    * Reinforcement learning
    * Adversarial training

* 2 Related Work

* 3 Proposed Method
    * 3.1 Problem Definition
    * 3.2 Graph Generation as Markov Decision Process
    * 3.3 Molecule Generation Environment
        * State Space
        * Action Space
        * State Transition Dynamics
        * Reward design
    * 3.4 Graph Convolutional Policy Network
        * Computing node embeddings
        * Action prediction
    * 3.5 Policy Gradient Training

* 4 Experiments
    * Property Optimization
    * Property Targeting
    * Constrained Property Optimization
    * 4.1 Experimental Setup
        * Dataset
        * Molecule environment
        * GCPN Setup
        * Baselines

    * 4.2 Molecule Generation Results
        * Property optimization
        * Property Targeting
        * Constrained Property Optimization

* 5 Conclusion

---
---

## Abstract
기초적인 규칙을 지키면서도 새로운 그래프 구조를 만들어 내는 것은 화학, 생물학 등에서 중요하다. 특히, 이 논문처럼 그럴듯하면서도 새로운 분자 그래프를 만들어 내는 것은 신약개발에서 더욱 중요하다. 그럴듯 하다는것은 drug-likeness를 가져야 할 뿐 아니라, chemical valency같은 물리적인 규칙도 만족시켜야 한다는 것을 의미한다. 그러나 원하는 특성을 가진 분자를 찾는 모델을 만드는 것은 굉장히 복잡하고 미분불가능한 규칙때문에 어려웠다. 본 논문에서는 Graph Convolutional Policy Network(GCPN)을 제안한다. Graph convolutional network와 강화학습(policy gradient)을 이용하여 목적에 맞게 그래프를 만들어준다. reward를 domain에 맞게 주고, policy gradient를 통해 adversarial loss를 준다. 최적화 성능이 state-of-the-art보다 chemical property에서 61%, constrained property에서 184% 올랐다고 한다.

## Introduction
신약개발이나 재료과학에서 원하는 특성을 갖는 새로운 분자구조를 설계하는 것이 중요하다. 그러나, 탐색해야할 chemical space가 어마어마하게 크기때문에 굉장히 어려웠다. 게다가 chemical space는 불연속적이고 작은 변화에도 분자 특성이 크게 변한다.  

최근 딥러닝을 적용시켜 많은 발전이 있긴 했지만, 새로운 약물 구조를 만들어 물리적, 화학적, 생물학적 특성을 고루 만족시키는 것은 여전히 어려운 일이다. 또한 데이터셋에서 존재하는 분자의 분포랑 우리가 원하는 특성을 갖는 분자의 분포랑 다르기 때문에 결국 막대한 chemical space를 탐색해야 한다.

* **Present Work**:
    여기서는 chemical rule을 지키도록 제한을 두면서 새로운 분자구조를 만들어내는 Graph Convolutional Policy Network (GCPN)를 제안한다. 크게 세가지 아이디어를 하나의 프레임워크로 묶어서 사용했는데, 이어서 설명할 **graph representation**, **reinforcement learning**, **adversarial training**이다.

* **Graph representation** :
    그래프 구조의 vector repersentation을 만들기 위해 사용된다. 기존의 simplifed molecular-input line-entry system(SMILES)나 텍스트 기반 임베딩 보다 robust 하다. 예를 들면, text-based representation에서는 글자 하나만 바뀌어도 엄청난 변화가 생겨서 쓸모없게 되는 경우가 있다. 게다가, 생성된 분자의 일부분도 substructure로써 의미를 갖고 해석되는 경우가 있는데, text representation는 일부분만 봐서는 대개 의미가 없다. 그러나 그래프에서는 부분적으로 생성된 분자에 대해서도 valency check같은 chemical check를 할 수 있다.
    ![GCPN-01](/assets/img/Paper/GCPN/GCPN-01.png)
    *text based representation (seq-to-seq model) https://arxiv.org/pdf/1706.01643.pdf*

* **Reinforcement learning** :
    Reinforcement learning(RL)을 이용하는 것은 generative model을 사용하는 것보다 몇가지 이점이 있다.
    1. drug-likeness나 valency 같은, 제한적인 분자의 특성을 만족시켜야 한다. 그런데 이런 특성인 복잡하고 non-differentiable 하기 때문에, 하나의 generative network의 objective function으로 묶어서 학습시킬 수 없다. 반면에 강화학습은 environment dynamics와 reward function의 설계에 따라 hard constraint와 desired property들을 표현할 수 있다.
    2. 기존 generative model들은 학습 데이터셋의 범위 안에서 분자 그래프를 만들어 내는 반면에, 강화학습은 데이터셋 이외의 영역도 탐색한다(exploration).


* **Adversarial training** : 
    데이터셋에 있는 분자에 대한 사전지식을 반영하는 것은 분자 생성에 매우 중요하다. 예를 들어, 약물은 보통 생리학적으로 안정적이고 비독성이다. 하나의 특성을 위해서 rule-based로 일일이 설정해주는 것은 가능하긴 하지만, 여러가지 특성의 조합을 위한 작업은 매우 어렵다. Adversarial loss를 통해 discriminator을 학습시킴으로써 데이터셋의 정보를 implicitly 학습하고 generator의 학습도 가이드한다. Discriminator는 graph convolutional network 구조로 이루어져있다. ([Semi-supervised classification with graph convolutional networks](https://arxiv.org/abs/1609.02907), [Convolutional networks on graphs for learning molecular fingerprints](https://hips.seas.harvard.edu/files/duvenaud-graphs-nips-2015.pdf))

GCPN은 chemistry-aware graph generation environment에서 학습하는 RL agent이다. 새로운 substructure이나 atom을 기존의 분자 그래프에 이어주거나, 기존의 atom들끼리 이어주는 방식으로 분자그래프를 생성한다. 

molecule property optimization, property targeting 그리고 conditional property
optimization 이 세가지의 테스크에 대해 실험한다. ZINC dataset을 사용하였다. 

## Related Work
* [Yang et al][40] and [Olivecrona et al][30] : 각각 Monte Carlo tree search와 policy gradient를 이용해서 최적화한 RNN을 사용해서 SMILES string을 생성한다.
![GCPN-02](/assets/img/Paper/GCPN/GCPN-02.png)
*Yang et al(top) and Olivecrona et al(bottom)*

* [Guimaraes et al][26] and [Sanchez-Lengeling et al][33] : 학습 데이터셋의 분자와 비슷하도록 adversarial loss를 reinforcement learning의 reward에 적용시켰다.
![GCPN-03](/assets/img/Paper/GCPN/GCPN-03.png)
*Guimaraes et al(left) and Sanchez-Lengeling et al(right)*


위 논문들에서는 text-based representation을 사용했는데, 본 논문에서는 graph representation을 사용함으로써 위에서 언급한 이점들을 취했다.

* [Jin et al][15] : Variational autoencoder(VAE)을 사용했다. 분자구조를 atome들의 작은 클러스터들로 이루어진 junction tree로 나타냈다.
![GCPN-04](/assets/img/Paper/GCPN/GCPN-04.png)
*Jin et al*

위 논문에서는 latent vector을 학습시켜 간접적으로 분자 특성을 최적화 하는 반면, 본 논문에서는 분자특성을 직접적으로 최적화 한다.

* [You et al][41] : Auto-regressive model을 이용해서 그래프 생성 프로세스를 최적화 하지만, 원하는 그래프를 만들어낼 수는 없다.
![GCPN-05](/assets/img/Paper/GCPN/GCPN-05.png)
*You et al*

* [Yujia Li et al][24] and [Yibo Li et al][25] : 이 논문들에서는 원하는 분자 특성을 가진 sequential graph 생성 모델을 제안하지만 마찬가지로 분자 특성을 직접적으로 최적화하지는 못한다.
![GCPN-06](/assets/img/Paper/GCPN/GCPN-06.png)
*Yujia Li et al(top) and Yibo Li et al(bottom)*



## Proposed Method

#### Problem Definition
그래프 $G$ 를 표현하는 표기는 $(A,E,F)$ 로 한다. 각각의 노드마다 $d$차원 feature을 갖고 있고, edge(결합)의 종류는 $b$종류이다.
* Adjacency matrix: $$A \in \left\{ 0,1 \right\}^{n \times n}$$
* Feature matrix: $$F \in \mathbb{R^{n \times d}}$$
* Edge: $$E \in \left\{ 0,1 \right\}^{b \times n \times n} $$

예를 들어, $E_{i,j,k} = 1$ 은 노드 $j$와 $k$ 사이에 $i$ 종류의 edge(결합)가 있다는 뜻이다. 이때 adjacency $A$는 $\sum_{i=1}^{b} E_i$ 이다.

생성되는 그래프는 $G'$ 이고, 하나 이상의 분자 특성을 $S$라고 할때, 그래프 $G'$의 타겟 특성 $$S\left(G'\right)$$이 최대화 되는 것을 목표로 한다. 즉, $$maximize  \mathbb{E}_{G'} \left[ S\left(G'\right) \right]$$ 이다. 여기에는 두가지 중요한 제한사항이 있다.
1. 생성된 그래프는 hard constraints를 만족시켜야 한다.
2. 위에서 언급한대로 기존 데이터 $G \sim p_{data}(G)$ 에 존재하는 (chemical valency같은)사전지식을 implicitly 학습하고 generator의 학습도 regularizing한다. distance metric을 $J(\centerdot,\centerdot)$이라 할 때, $$\mathbb{E}_{G,G'} \left[J(G,G')\right]$$을 최적화 하는 것이다.

[](){:name='figure1'}
![GCPN-07](/assets/img/Paper/GCPN/GCPN-07.png)
*Figure 1. An overview of the proposed iterative graph generation method*

**(a)** intermediate graph $G_t$에 Scaffold subgraph $C$를 더한다. **(b)** GCPN은 state 임베딩하기위해 message passing을 수행하고 policy $\pi_{theta}$ 를 만든다. **(c)** policy로부터 4개 component의 action $a_{t}$를 뽑는다. **(d)** 환경은 intermediate state에 대해 chemical valency check를
수행하고 **(e)** 다음 상태 $G_{t+1}$와 **(f)** reward $r_t$을 반환한다. 각 요소들은 아래에서 다시 설명된다.


#### Graph Generation as Markov Decision Process
Graph generation process를 $M = (\mathcal{S,A,P,R,}\gamma)$로 정의한다. $$\mathcal{S} = \left\{ s_i \right\}$$ 는 모든 가능한 그래프의 상태 집합이다. $$\mathcal{A} = \left\{ a_i \right\}$$ 는 각각 타임 스텝마다 그래프를 만들기 위한 action들의 집합이다. $$\mathcal{P}$$는 action $a_t$를 했을 때 특정 state로 가는 transition distribution $p(s_{t+1}\|s_t,...,s_0,a_t)$이다. $\mathcal{R}(s_t)$는 state $s_t$에 도달한 후에 받는 reward를 내뱉는 reward function이다. $\gamma$는 타임 스텝마다 reward를 감소시키는 discount factor이다. 그래프를 생성하는 과정은 $(s_0,a_0,r_0,...,s_n,a_n,r_n)$ 로 나타낼 수 있다. 

여기서는 graph generation 을 Markov Decision Process(MDP)로 정의한다. MDP는 상태 전이함수가 Markov property를 만족시킨다. 
![CS294-02-03](/assets/img/Deeplearning/CS294/02/CS294-02-03.png)
[*강화학습 용어, CS294*]({{ site.url }}/deep%20learning/2018/09/03/CS294-02/#term)
* Markov property : 
state $ s_{t} $에서 action $ a_{t} $ 를 취했을때 $ s_{t+1} $ 로 가는 확률을 $ p(s_{t+1}|s_{t},a_{t})$ 로 나타낸다. 이때 $ s_{t}$ 에서 $ s_{t+1}$ 로 갈때 $ s_{t-1} $과는 독립적이다. 이를 Markov property라고 한다. 다시말해, 과거 상태 $ s_{t-1} $와 현재 상태 $ s_{t} $가 주어졌을 때의 미래 상태의 조건부 확률 분포 $ p(s_{t+1}|s_{t},a_{t}) $가 과거 상태와는 독립적으로 현재 상태 $ s_{t} $에 의해서만 결정된다는 것을 뜻한다.

여기서 policy network는 intermediate graph state $s_t$만을 input으로 받고 output으로 action을 내뱉는다. action은 환경이 다음 intermediate graph를 생성하는데 사용된다. 

#### Molecule Generation Environment
환경은 GCPN으로부터 action을 받아 state를 업데이트하고, reward를 준다.

* **State Space** :  time step $t$ 에서의 그래프 $G_t$를 state $S_t$로 정의한다. [Figure 1](#figure1)의 (a)는 action 전, (e)는 action 후의 state 이다. 초기 state $s_0$ 즉, 그래프 $G_0$는 하나의 carbon atom 으로 가정한다.

* **Action Space** : 






* **State Transition Dynamics** : 



* **Reward Design** : 



























[15]: https://arxiv.org/abs/1802.04364 "Junction tree variational autoencoder for molecular graph generation"
[24]: https://arxiv.org/abs/1803.03324 "Learning Deep Generative Models of Graphs"
[25]: https://arxiv.org/abs/1801.07299 "Multi-Objective De Novo Drug Design with Conditional Graph Generative Model"
[26]: https://arxiv.org/abs/1705.10843 "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models"
[30]: https://arxiv.org/abs/1704.07555 "Molecular de-novo design through deep reinforcement learning"
[33]: https://chemrxiv.org/articles/ORGANIC_1_pdf/5309668 "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry"
[40]: https://arxiv.org/abs/1710.00616 "An Efficient Python Library for de novo Molecular Generation"
[41]: https://arxiv.org/abs/1802.08773 "Graphrnn: A deep generative model for graphs"




