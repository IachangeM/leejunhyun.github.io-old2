---
layout: post
title: '(작성중)[CS294] Lecture 4 : Reinforcement Learning Introduction'
categories:
  - Deep Learning
tags:
  - deep learning
  - reinforcement learning
  - CS294
  - course review
excerpt_separator: <!--more-->
---


UC Berkeley의 강화학습 강의 CS294(2018 Fall semester)를 정리한 포스트입니다.
<!--more-->

* [Course website](http://rail.eecs.berkeley.edu/deeprlcourse/)
* [Youtube video](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)
* [Reddit](https://www.reddit.com/r/berkeleydeeprlcourse/)


코스 전체에서 다루는 주제입니다. (자세한 항목은 syllabus 참조하시기 바랍니다.)
1. From supervised learning to decision making
2. Model-free algorithms: Q-learning, policy gradients, actor-critic
3. Advanced model learning and prediction
4. Exploration
5. Transfer and multi-task learning, meta-learning
6. Open problems, research talks, invited lectures

---
---
## 포스트 내용
1. Definition of a Markov decision process
2. Definition of reinforcement learning problem
3. Anatomy of a RL algorithm
4. Brief overview of RL algorithm types
* Goals:
  * Understand definitions & notation
  * Understand the underlying reinforcement learning objective
  * Get summary of possible algorithms

---
## Definitions
#### Notations from imitation learning
> *Imitation Learning [(Lecture 2 강의 내용)]({{site.url}}/deep%20learning/2018/09/03/CS294-02/#term) 중...*

![CS294-02-01](/assets/img/Deeplearning/CS294/02/CS294-02-01.png)
기존 지도학습중에서 Classification task를 강화학습의 표기로 이어서 설명한다.
<p>매 시각 \( t \)마다 관측과 분류를 한다고 가정하면, 기존 이미지 input은 관측(observation) \( o_{t} \)로 표기되고, 동물의 종류를 구분하는 대신 agent의 행동(action)을 구분한다면 output은 \( a_{t} \)가 된다. observation \( o_{t} \)가 주어졌을 때 action \( a_{t} \)의 확률분포를 내뱉는 것은 policy \( \pi_\theta(a_{t}|o_{t}) \)이고, \( \theta \)는 뉴럴넷과 같은 파라미터가 된다. \(o_{t}\)는 state \(s_{t}\)로부터 나온다. </p>

![CS294-02-02](/assets/img/Deeplearning/CS294/02/CS294-02-02.png)
<p>
\( o_{t} \)와 \( s_{t} \)의 관계를 좀 더 설명하면, 위 그림을 보면 영양이 치타에게 쫓기고 있는데, 이 사진은 픽셀로 표현된 관측 \( o_{t} \)이고, state \( s_{t} \)는 두 객체의 (역학적) 상태라고 볼 수 있다. 우리는 똑똑하게도 사진 \( o_{t} \)만 보고도 state \( s_{t} \)를 알 수 있지만 기계는 그렇지 못하다. (학습을 해야한다.) 이 때 누군가 운전해와서 자동차가 치타를 가리게되면 관측 \( o_{t} \)는 변하지만 치타와 영양의 상태 \( s_{t} \)는 변하지 않는다.
</p>

![CS294-02-03](/assets/img/Deeplearning/CS294/02/CS294-02-03.png)
<p>
따라서 state \( s_{t} \), observation \( o_{t} \) 그리고 action \( a_{t} \) 의 관계를 그래프로 그려보면 위와같다. 그리고 state \( s_{t} \)에서 action \( a_{t} \)를 취했을때 \( s_{t+1} \)로 가는 확률을 \( p(s_{t+1}|s_{t},a_{t}) \)로 나타낸다. 이때 \( s_{t} \)에서 \( s_{t+1} \)로 갈때 \( s_{t-1} \)과는 독립적이다. 이를 Markov property라고 한다. 다시말해, 과거 상태 \( s_{t-1} \)와 현재 상태 \( s_{t} \)가 주어졌을 때의 미래 상태의 조건부 확률 분포 \( p(s_{t+1}|s_{t},a_{t}) \)가 과거 상태와는 독립적으로 현재 상태 \( s_{t} \)에 의해서만 결정된다는 것을 뜻한다.
</p>

![CS294-02-04](/assets/img/Deeplearning/CS294/02/CS294-02-04.png)
<p>
운전하는 사람으로부터 observation \( o_{t} \)와 사람의 action \( a_{t} \)의 데이터를 쌓아서, \( \pi_\theta(a_{t}|o_{t}) \) 를 지도 학습하는 것을 behavior cloning이라고 한다.
<strong>일반적으로는 잘 안된다.</strong> training data에 없던 data가 test에 나온다던지, 애초에 학습데이터의 사람의 action이 bad action이었던지, 사람이 비슷한 상황 \( o_{t} \)에 대해 서로 다른 action \( a_{t} \)를 취한다던지, 여러 이유가 있을 수 있다.
</p>

#### Reward functions
> 어떤 행동이 좋고, 어떤 행동이 나쁜가?

이에 대한 기준이 reward function $r(s,a)$ 이다. state와 action이 더 나은지에 대한 함수다. 만약 자율주행차가 길을 잘 달리면 높은 reward를 받을 것이고, 사고가 난다면 낮은 reward를 받을 것이다.

#### Markov chain



















