---
layout: post
title: '(작성중)[CS294] Lecture 4 : Reinforcement Learning Introduction'
categories:
  - Deep Learning
tags:
  - deep learning
  - reinforcement learning
  - CS294
  - course review
excerpt_separator: <!--more-->
---


UC Berkeley의 강화학습 강의 CS294(2018 Fall semester)를 정리한 포스트입니다.
<!--more-->

* [Course website](http://rail.eecs.berkeley.edu/deeprlcourse/)
* [Youtube video](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)
* [Reddit](https://www.reddit.com/r/berkeleydeeprlcourse/)


코스 전체에서 다루는 주제입니다. (자세한 항목은 syllabus 참조하시기 바랍니다.)
1. From supervised learning to decision making
2. Model-free algorithms: Q-learning, policy gradients, actor-critic
3. Advanced model learning and prediction
4. Exploration
5. Transfer and multi-task learning, meta-learning
6. Open problems, research talks, invited lectures

---
---
## 포스트 내용
1. Definition of a Markov decision process
2. Definition of reinforcement learning problem
3. Anatomy of a RL algorithm
4. Brief overview of RL algorithm types
* Goals:
  * Understand definitions & notation
  * Understand the underlying reinforcement learning objective
  * Get summary of possible algorithms

---
## Definitions
#### Notations from imitation learning
> *Imitation Learning [(Lecture 2 강의 내용)]({{site.url}}/deep%20learning/2018/09/03/CS294-02/#term) 중...*

![CS294-02-01](/assets/img/Deeplearning/CS294/02/CS294-02-01.png)
기존 지도학습중에서 Classification task를 강화학습의 표기로 이어서 설명한다.
<p>매 시각 \( t \)마다 관측과 분류를 한다고 가정하면, 기존 이미지 input은 관측(observation) \( o_{t} \)로 표기되고, 동물의 종류를 구분하는 대신 agent의 행동(action)을 구분한다면 output은 \( a_{t} \)가 된다. observation \( o_{t} \)가 주어졌을 때 action \( a_{t} \)의 확률분포를 내뱉는 것은 policy \( \pi_\theta(a_{t}|o_{t}) \)이고, \( \theta \)는 뉴럴넷과 같은 파라미터가 된다. \(o_{t}\)는 state \(s_{t}\)로부터 나온다. </p>

![CS294-02-02](/assets/img/Deeplearning/CS294/02/CS294-02-02.png)
<p>
\( o_{t} \)와 \( s_{t} \)의 관계를 좀 더 설명하면, 위 그림을 보면 영양이 치타에게 쫓기고 있는데, 이 사진은 픽셀로 표현된 관측 \( o_{t} \)이고, state \( s_{t} \)는 두 객체의 (역학적) 상태라고 볼 수 있다. 우리는 똑똑하게도 사진 \( o_{t} \)만 보고도 state \( s_{t} \)를 알 수 있지만 기계는 그렇지 못하다. (학습을 해야한다.) 이 때 누군가 운전해와서 자동차가 치타를 가리게되면 관측 \( o_{t} \)는 변하지만 치타와 영양의 상태 \( s_{t} \)는 변하지 않는다.
</p>

![CS294-02-03](/assets/img/Deeplearning/CS294/02/CS294-02-03.png)
<p>
따라서 state \( s_{t} \), observation \( o_{t} \) 그리고 action \( a_{t} \) 의 관계를 그래프로 그려보면 위와같다. 그리고 state \( s_{t} \)에서 action \( a_{t} \)를 취했을때 \( s_{t+1} \)로 가는 확률을 \( p(s_{t+1}|s_{t},a_{t}) \)로 나타낸다. 이때 \( s_{t} \)에서 \( s_{t+1} \)로 갈때 \( s_{t-1} \)과는 독립적이다. 이를 Markov property라고 한다. 다시말해, 과거 상태 \( s_{t-1} \)와 현재 상태 \( s_{t} \)가 주어졌을 때의 미래 상태의 조건부 확률 분포 \( p(s_{t+1}|s_{t},a_{t}) \)가 과거 상태와는 독립적으로 현재 상태 \( s_{t} \)에 의해서만 결정된다는 것을 뜻한다.
</p>

![CS294-02-04](/assets/img/Deeplearning/CS294/02/CS294-02-04.png)
<p>
운전하는 사람으로부터 observation \( o_{t} \)와 사람의 action \( a_{t} \)의 데이터를 쌓아서, \( \pi_\theta(a_{t}|o_{t}) \) 를 지도 학습하는 것을 behavior cloning이라고 한다.
<strong>일반적으로는 잘 안된다.</strong> training data에 없던 data가 test에 나온다던지, 애초에 학습데이터의 사람의 action이 bad action이었던지, 사람이 비슷한 상황 \( o_{t} \)에 대해 서로 다른 action \( a_{t} \)를 취한다던지, 여러 이유가 있을 수 있다.
</p>

#### Reward functions
> 어떤 행동이 좋고, 어떤 행동이 나쁜가?

이에 대한 기준이 reward function $r(s,a)$ 이다. state와 action이 얼마나 좋은지에 대한 함수다. 만약 자율주행차가 길을 잘 달리면 높은 reward를 받을 것이고, 사고가 난다면 낮은 reward를 받을 것이다.

#### Markov chain
* $\mathcal{M} = \{\mathcal{S,T}\}$
* $\mathcal{S}$ - state space
    * states $s \in \mathcal{S}$ (discrete or continuous)
* $\mathcal{T}$ - Transition operator(Matrix)
    * $p (s_{t+1}\|s_t)$
    * 이전 state에서 다음 state로 갈때(상태가 전이될 때)의 확률분포를 나타낸다. 상태가 전이 될 때는 오직 이전 상태에만 영향을 받는다.

![CS294-04-01](/assets/img/Deeplearning/CS294/04/CS294-04-01.png)

#### Markov decision process
Markov chain 는 state밖에 없었는데, 여기에 action과 reward가 추가된다.
* $\mathcal{M} = \{\mathcal{S,A,T,r}\}$
* $\mathcal{S}$ - state space
    * states $s \in \mathcal{S}$ (discrete or continuous)
* $\mathcal{A}$ - action space
    * actions $a \in \mathcal{A}$ (discrete or continuous)
* $\mathcal{T}$ - Transition operator(Tensor!)
    * $p (s_{t+1}\|s_t,a_t)$
* $r$ - reward function
    * $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
    * $r(s_t,a_t)$ - reward

![CS294-04-02](/assets/img/Deeplearning/CS294/04/CS294-04-02.png)

#### Partially observed Markov decision process
대부분의 강화학습 환경에서는 agent가 state를 모두 보는 것이 아니라, 부분적으로만 관찰할 수밖에 없는 경우가 많다. 위의 Markov decision process 에서 observation과 emission probability(특정 state에서 특정 observation이 나올 확률)가 추가된다.
* $\mathcal{M} = \{\mathcal{S,A,O,T,\varepsilon,r}\}$
* $\mathcal{S}$ - state space
    * states $s \in \mathcal{S}$ (discrete or continuous)
* $\mathcal{A}$ - action space
    * actions $a \in \mathcal{A}$ (discrete or continuous)
* $\mathcal{O}$ - observation space
    * observations $o \in \mathcal{O}$ (discrete or continuous)
* $\mathcal{T}$ - Transition operator(Tensor!)
    * $p (s_{t+1}\|s_t,a_t)$
* $\varepsilon$ - emission probability $p(o_t\|s_t)$
* $r$ - reward function
    * $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
    * $r(s_t,a_t)$ - reward


![CS294-04-03](/assets/img/Deeplearning/CS294/04/CS294-04-03.png)

#### The goal of reinforcement learning

![CS294-04-04](/assets/img/Deeplearning/CS294/04/CS294-04-04.png)
$\tau$ 는 state와 action의 trajectory $s_1,a_1,...,s_T,a_T$이고, $\pi_{\theta}$는 state $s_t$를 입력으로 받아 action $a_t$를 내뱉는 policy network이다. state와 policy가 내뱉은 action을 world(환경)에 집어 넣으면 transition operator $p(s_{t+1}\|s_t,a_t)$에 의해 다음 state $s_{t+1}$가 나오고 $s_{t+1}$을 policy network에 넣으면 action $a_{t+1}$이 나온다. 그리고 $\theta$는 policy network의 학습 파라미터이다. 우리는 이 때, 최적의 파라미터 $\theta^{\star}$를 찾는 것이 강화학습의 목표다. 최적이라는 것은, reward를 가장 많이 받는 것이므로, 결국 policy를 따라 행동했을 때의 reward 기댓값 $\mathbb{E}\_{\tau \sim p\_{\theta}(\tau)} \left[ \underset{t}{\sum} r(s\_t,a\_t) \right]$ 을 최대로 하는 $\theta^{\star}$를 찾는 것이다. 


#### Finite horizon case: state-action marginal

















